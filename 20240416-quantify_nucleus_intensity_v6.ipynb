{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e7c3c06",
   "metadata": {},
   "source": [
    "# Trace blobs (e.g. nuclei, cells, ...) over the z-axis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06259e5",
   "metadata": {},
   "source": [
    "Code written and conceptualized largely by Nathan De Fruyt (nathan.defruyt@kuleuven.be, nathan.defruyt@gmail.com) with initial leads by Wim Thiels (wim.thiels@kuleuven.be) in the Beets and Jelier labs. \n",
    "\n",
    "The algorithm is simple, but functional for minimal goals:\n",
    "1. **version 3 edit:** instead of blob recognition, I threshold on the percentile of I values (i.e. also intrinsic background correction)\n",
    "1. **version 4 edit:** splitting blobs that are larger than normal (95% percentile?)\n",
    "1. **version 5 edit:** still to figure out splitting, but already 3D image labeling + better thresholding\n",
    "1. **version 6 edit:** segment anything algorithm instead of skimage\n",
    "\n",
    "\n",
    "\n",
    "* next, I deviated from Wim's advice and went on to work with the blob's \n",
    "    1. **center coordinates\n",
    "    1. **mean intensity value\n",
    "    1. **radius** (restricted to 20 pixels, as this appeared to be towards the higher end of nucleus radii -- adapt this!)\n",
    "\n",
    "To this, the program:\n",
    "1. determines **common blob labels** based on how near they are (max displacement of center = 10 pixels in x/y direction, max rise of 5 planes)\n",
    "1. renders one line per blob for the plane with the **highest intensity value**\n",
    "\n",
    "Each step (1. blob identification, 2. blob labelling, 3. summary) are rendered in separate .csv files. \n",
    "Blob identification takes the longest (a few hours for a day of pictures). The subseding steps are fast.\n",
    "\n",
    "**Parameters can therefore be adapted** in the second and third step without consideration. \n",
    "\n",
    "Do think about changing parameters to the first step.\n",
    "\n",
    "___Questions are welcome, optimization of the algorithm too.___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e89e3518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating analysis using arguments: \n",
      "minimum radius = 2,\n",
      "maximal radius = 40,\n",
      "threshold = 95th percentile,\n",
      "background is calculated as mean of all values in the 20th percentile\n",
      "all objects that have a length/width ratio > 0.6666666666666666 will be split in two\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "min_radius = 2\n",
    "max_radius = 40\n",
    "threshold = 95\n",
    "background_threshold = 20\n",
    "split_ratio = 2/3\n",
    "\n",
    "# threshold, background_threshold, min_radius, max_radius, split_ratio = list(map(float, sys.argv[1:]))\n",
    "print(f'Initiating analysis using arguments: \\nminimum radius = {min_radius},\\nmaximal radius = {max_radius},\\nthreshold = {threshold}th percentile,\\nbackground is calculated as mean of all values in the {background_threshold}th percentile\\nall objects that have a length/width ratio > {split_ratio} will be split in two')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e58bbf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "## general math and system modules/functions\n",
    "from math import sqrt, atan, tan, cos, sin\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import itertools\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from tqdm import tqdm\n",
    "import shutil as sh\n",
    "\n",
    "## parallellization!\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "## data formatting!\n",
    "import pandas as pd\n",
    "\n",
    "## image import and processing module functions\n",
    "import czifile as cfile\n",
    "from skimage import data, data, measure, exposure\n",
    "from skimage.measure import label, regionprops_table, regionprops\n",
    "from skimage.morphology import closing\n",
    "from skimage.segmentation import clear_border\n",
    "from skimage.feature import blob_dog, blob_log, blob_doh\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.filters import gaussian, laplace, threshold_otsu\n",
    "import cv2 as cv\n",
    "import PIL\n",
    "import tifffile as tf\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "import string\n",
    "\n",
    "## plotting modules\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import Axes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05888dd0",
   "metadata": {},
   "source": [
    "## 1. Find data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463ebb3a",
   "metadata": {},
   "source": [
    "First I adapted some existing functions to more easily check and handle data either here in jupyter notebook or to the purpose of an application. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a49bd0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some easier to handle functions for reading image data\n",
    "def readczi(filepath):\n",
    "    img = cfile.imread(filepath)\n",
    "    return(img[0, 0, :img.shape[2], :img.shape[3], :img.shape[4], 0])\n",
    "\n",
    "# def showplane(img, z):\n",
    "#     plt.imshow(img[z, :img.shape[1], :img.shape[2]])\n",
    "    \n",
    "# def getplane(img, z):\n",
    "#     return(img[z, :img.shape[1], :img.shape[2]])\n",
    "\n",
    "# def getyplane(img, y):\n",
    "#     return(img[:img.shape[0], :img.shape[1], y])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb10ea9",
   "metadata": {},
   "source": [
    "## 2. Render ___summary___ (mean, max, ...) intensity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211cd96f",
   "metadata": {},
   "source": [
    "#### notes to self: \n",
    "1. parallellize?**\n",
    "1. threshold on min_radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc7de738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_imip(filepath, min_radius, min_z_radius):\n",
    "#     ## trace back czi file path\n",
    "#     im_filepath = filepath[:-len('_max_intensity_sum.csv')] + '.czi'\n",
    "    \n",
    "#     ## read in czi file\n",
    "#     imstack = readczi(im_filepath)\n",
    "    \n",
    "#     ## \n",
    "#     max_intensity_df = pd.read_csv(filepath).drop('Unnamed: 0', axis = 1)\n",
    "#     max_intensity_df = max_intensity_df.rename(columns = {'x': 'y', 'y': 'x'})\n",
    "#     max_intensity_df = max_intensity_df[max_intensity_df['label'].isin(max_intensity_df.label[max_intensity_df['r'] > min_radius])] ## \n",
    "#     max_intensity_df = max_intensity_df[max_intensity_df['label'].isin(max_intensity_df.label[max_intensity_df['r_z'] > min_z_radius])] ## \n",
    "#     # inverted mean intensity projection - DO NOT USE FOR QUANTIFICATION!\n",
    "    \n",
    "#     return([os.path.basename(im_filepath), np.max(imstack) - np.mean(imstack, axis=0), max_intensity_df])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1149c9b2",
   "metadata": {},
   "source": [
    "**To find the transformed center coordinates of the split object**:\n",
    "\n",
    "1. We **split** evenly **along the major axis**\n",
    "1. The new center coordinates follow:\n",
    "    * $dx = r / sqrt(1+s^{2})$\n",
    "    \n",
    "    * $dy = r/sqrt(1+1/s^{2})$\n",
    "    \n",
    "    \n",
    "1. The new areas are circles with the minor axis as radius\n",
    "1. We include only those pixels that were within the original area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c99995e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def extract_img_features(img_bg, img_lbl, bg, split_ratio = 2/3):\n",
    "#     Idf = pd.DataFrame(regionprops_table(label_image=img_lbl, intensity_image=img_bg, properties = ('label', 'intensity_mean', 'area', 'centroid', 'bbox', 'axis_major_length', 'axis_minor_length', 'orientation')))\n",
    "#     ## relabel columns\n",
    "#     Idf = Idf.rename(columns = {'label': \"ID\", 'intensity_mean': \"I\", 'area': \"area\", 'centroid-0': \"y\", 'centroid-1': \"x\", 'bbox-0': \"y_min\", 'bbox-1': \"x_min\", 'bbox-2': \"y_max\", 'bbox-3': \"x_max\", 'axis_major_length': \"d_max\", 'axis_minor_length': \"d_min\", 'orientation': 'slope'})\n",
    "#     ## add a column for ...\n",
    "#         ## ... r(adius)...\n",
    "#     Idf.insert(5, 'r', (Idf['d_max'] + Idf['d_min'])/4)\n",
    "#         ## ...b(ack)g(round)...\n",
    "#     Idf.insert(2, 'bg', bg)\n",
    "#         ## ...min(imal circle).area...\n",
    "#     Idf.insert(4, 'min.area', np.pi*(Idf['d_min']/2)**2)\n",
    "#         ## ...area.ratio (actual area/minimal area)...\n",
    "#     Idf['area.ratio'] = Idf['min.area']/Idf['area']\n",
    "#         ## ...r(adius).ratio (minor axis diameter/major axis diamter)...\n",
    "#     Idf['r.ratio'] = Idf['d_min']/Idf['d_max']\n",
    "\n",
    "#         ## ... whether to split the object based on r-ratio and/or min area? \n",
    "#     Idf['split'] = 0\n",
    "#     for i in Idf.index:\n",
    "#         Idf.loc[i, 'split'] = 1 if ((Idf.loc[i, 'r.ratio'] < split_ratio) | (Idf.loc[i, 'area.ratio'] < split_ratio)) else 0\n",
    "    \n",
    "#     ## return dataframe\n",
    "#     return(Idf)\n",
    "\n",
    "# def split_objects(ID, Idf, img_lbl):\n",
    "#     ## shorten name\n",
    "#     sub = Idf[Idf['ID'] == ID]\n",
    "\n",
    "#     ## and extract some parameters to make code more readable\n",
    "#     x = sub.x\n",
    "#     y = sub.y\n",
    "\n",
    "#     r = float(sub.d_max)/4\n",
    "#     s = sub.slope\n",
    "\n",
    "#     ## calculate deviation of new coordinates from original center coordinates (see cell above for explanation)\n",
    "#     dx = r/sqrt(1+s**2)\n",
    "#     dy = r/sqrt(1+ 1/(s**2))\n",
    "\n",
    "#     ## set new coordinates (subtract and add coordinates to origin coordinates)\n",
    "#     x1 = float(x - np.sign(s)*dx)\n",
    "#     x2 = float(x + np.sign(s)*dx)\n",
    "#     y1 = float(y + np.sign(s)*dy)\n",
    "#     y2 = float(y - np.sign(s)*dy)\n",
    "\n",
    "#     origins = [(x1, y1), (x2, y2)]\n",
    "\n",
    "#     # ## mainly, compose output dataframe:\n",
    "#     Idf_newlines = Idf.loc[Idf['ID'] == ID, :]\n",
    "\n",
    "#     ## relabel in image\n",
    "#     alphabet = list(string.ascii_lowercase) ## to add an a/b to the original label\n",
    "#     for i, origin in enumerate(origins):\n",
    "#         ### get origin coordinates\n",
    "#         x0, y0 = origin\n",
    "\n",
    "#         ### a function to define whether a coordinate is within the circle (euclidean distance < radius)\n",
    "#         def in_circle(coord, origin, radius):\n",
    "#             v = abs(pdist([coord, origin],  metric = 'euclidean')[0])\n",
    "#             return(v < radius)\n",
    "\n",
    "#         ### make all possible coordinates within a square around the blob center\n",
    "#         pxls = list(itertools.product(list(range(int(sub.y_min), int(sub.y_max))), list(range(int(sub.x_min), int(sub.x_max)))))\n",
    "#         ###  select on those pixels that are within radius from the origin\n",
    "#             #### all pixels within the bounding box of the object\n",
    "#         pxls = np.array(pxls)[list(map(lambda c: in_circle(c, (y0, x0), float(sub.d_min)/2), pxls))]\n",
    "#             #### coordinates of labels for ID \n",
    "#         lbl_coords = np.transpose(np.where(img_lbl == ID))\n",
    "#             #### coordinates of labels to relabel as subsplit object label\n",
    "#         rlbl_coords = pxls[list(map(lambda pxl: pxl in lbl_coords, pxls))]\n",
    "#         if len(rlbl_coords) > 0:\n",
    "#             rows, cols = zip(*rlbl_coords)\n",
    "#             ### set new label\n",
    "#             img_lbl[rows, cols]= ID*10 + i + 1\n",
    "        \n",
    "#     img_lbl[img_lbl == ID] = 0\n",
    "\n",
    "#     return(img_lbl)\n",
    "\n",
    "# def extract_and_split_features_from_img(img, threshold, background_threshold, split_ratio = 2/3, min_radius = 10, max_radius = 30):\n",
    "    \n",
    "#     ## I. Threshold image\n",
    "    \n",
    "#     ## threshold image on percentile\n",
    "#     thresh = np.percentile(img, threshold)\n",
    "#     img_thresh = img*(img > thresh)\n",
    "\n",
    "#     ## calculate background signal as signal below background_threshold percentile\n",
    "#     bg = np.mean(img[(img < np.percentile(img, background_threshold))])\n",
    "\n",
    "#     ## subtract background signal for intenstiy calculations\n",
    "#     img_bg = img_thresh - bg\n",
    "#     img_bg[img_bg < 0] = 0\n",
    "#     img_bw = img > thresh\n",
    "\n",
    "#     ## smoothen surface\n",
    "#     img_cls = gaussian(closing(img_bw))\n",
    "\n",
    "#     # label adhering surfaces as blobs\n",
    "#     img_lbl = label(img_cls)\n",
    "\n",
    "#     ## II. extract features ####\n",
    "    \n",
    "#     ### 1) extract features from initial labels\n",
    "# #     print(f'>>> extracting object features')\n",
    "#     Idf = extract_img_features(img_bg, img_lbl, bg = bg, split_ratio = split_ratio)\n",
    "#     Idf = Idf[Idf['r'] > min_radius]\n",
    "#     Idf = Idf[Idf['r'] < max_radius]\n",
    "# #     print(f\">>> found {len(Idf[Idf['split']])} objects that we will split\")\n",
    "\n",
    "#     ### 2) plit objects that are at least 1.5 times longer than they are wide (ratio minor axis/major axis < 2/3 = area using min_radius < 2/3 of actual area)\n",
    "#     # only if there are objects to split:\n",
    "#     if len(Idf[Idf['split'] == 1]) > 0:\n",
    "#         ## iterate over objects to split\n",
    "#         for ID in Idf['ID'][Idf['split'] == 1]:\n",
    "#             ## subset data on object ID and relabel img_lbl\n",
    "#             img_lbl = split_objects(ID, Idf, img_lbl)\n",
    "# #             print(f\">>> split all ({len(Idf[Idf['split']])}) objects, re-calculating features\")\n",
    "\n",
    "#             ## recalcualte features\n",
    "# #             print(f\">>> split object {ID}, re-calculating features\")\n",
    "#             Idf = extract_img_features(img_bg, img_lbl, bg = bg, split_ratio = split_ratio)\n",
    "#             ## drop old label row\n",
    "# #             print(f\">>> dropping row {Idf.index[Idf['ID'] == ID]}\")\n",
    "#             Idf = Idf.drop(Idf.index[Idf['ID'] == ID])\n",
    "\n",
    "#     ## III. Return output data\n",
    "#     Idf.index = range(0, len(Idf.index))\n",
    "#     Idf = Idf[(Idf['r'] > min_radius)]\n",
    "#     Idf = Idf[(Idf['r'] < max_radius)]\n",
    "#     return(Idf, img_bg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0076a3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_and_Idf_from_imstack(imstack, sigma, threshold, split_ratio = 2/3):\n",
    "    \n",
    "    ## take laplacian of the gaussian of the image - make sigma wide enough for good smoothing\n",
    "    img_lp = laplace(gaussian(imstack, sigma = sigma))\n",
    "\n",
    "    ## threshold on percentile - very strict, yet slightly permissive threshold of 99.8\n",
    "    thresh = np.percentile(img_lp, threshold)\n",
    "    # otsu = threshold_otsu(img)\n",
    "    img_bw = img_lp > thresh\n",
    "\n",
    "    ## close the edges and label adhering regions\n",
    "    img_cls = closing(img_bw)\n",
    "    img_lbl = label(img_cls)\n",
    "\n",
    "    ## subtract the background from the image (everything that's below the threshold is considered background)\n",
    "    bg = np.mean(img_lp < thresh)\n",
    "    img_bg = imstack - bg\n",
    "    img_bg[img_bg < 0] = 0\n",
    "\n",
    "    ## now plot\n",
    "#     img_tp = img_lbl\n",
    "#     img_tp = np.mean(img_tp, axis = 0) - np.max(img_tp, axis = 0)\n",
    "#     plt.imshow(img_tp, cmap = 'gray')\n",
    "    \n",
    "    ## initiate an output dataframe\n",
    "#     Idf = pd.DataFrame([], columns = ['ID', 'I', 'bg', 'area', 'x', 'y', 'r', 'd_max', 'd_min', 'z', 'r_z', 'split'])\n",
    "\n",
    "    ## iterate over all z-planes\n",
    "#     for z in tqdm(range(0, imstack.shape[0])):\n",
    "#         ## get intensity data and the background-subtracted, thresholded image\n",
    "#         sub_Idf, plane = extract_and_split_features_from_img(imstack[z, :, :], threshold = threshold, background_threshold = background_threshold, split_ratio = split_ratio, min_radius = min_radius, max_radius = max_radius)\n",
    "\n",
    "#         ## add z\n",
    "#         sub_Idf['z'] = z\n",
    "\n",
    "#         ## save image and data\n",
    "#         Idf = pd.concat([Idf, sub_Idf], axis = 0)\n",
    "#         seq[z, :, :] = plane\n",
    "\n",
    "    ## measure features\n",
    "    Idf = pd.DataFrame(regionprops_table(label_image=img_lbl, intensity_image=img_bg, properties = ('label', 'intensity_mean', 'centroid', 'area')))\n",
    "    \n",
    "    ## relabel columns\n",
    "    Idf = Idf.rename(columns = {'label': \"ID\", 'intensity_mean': \"I\", 'area': \"area\", 'centroid-0': \"z\", 'centroid-1': \"y\", 'centroid-2': \"x\"})\n",
    "    \n",
    "    ## calculate radius\n",
    "    Idf['r'] = list(map(lambda x: np.cbrt(3*x/(4*np.pi)), Idf['area']))\n",
    "#     Idf['ID'] = range(0, len(Idf.index))\n",
    "#     Idf.index = Idf['ID']\n",
    "#     Idf['r'] = (Idf['r_x'] + Idf['r_y'])/2\n",
    "    \n",
    "    return(Idf, img_bg)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb38ae41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## to see the blobs\n",
    "# # def plot_blobs_on_inv_img(Idf, img):\n",
    "# #     ## inverted LUT image:\n",
    "# #     img_inv = np.max(img) - img\n",
    "\n",
    "# #     ## initiate subplots\n",
    "# #     fig, axes = plt.subplots(1, 1)\n",
    "    \n",
    "# #     ## iterate over blobs\n",
    "# #     for i in range(0, len(Idf)):\n",
    "# #         ## extract variables\n",
    "# #         ID, mean, sd, bg, r_x, x, r_y, y, size = Idf.iloc[i]\n",
    "# #         ## calculate radius\n",
    "# #         mean_r = np.mean([r_x, r_y])\n",
    "# #         ## plot the blob + a margin\n",
    "# #         c = plt.Circle((x, y), mean_r + .5*mean_r, color= 'black', linewidth=.3, linestyle = '--', fill=False)\n",
    "# #         axes.add_patch(c)\n",
    "# #         ## show the center of the blob\n",
    "# #         plt.text(x, y, '+', fontsize = 'xx-small', horizontalalignment='center', verticalalignment='center')\n",
    "# #         axes.imshow(img_inv, cmap = 'gray')\n",
    "    \n",
    "# #     ## show them on the figure\n",
    "# #     plt.tight_layout()\n",
    "# #     plt.show()\n",
    "    \n",
    "# # ## iterate over all planes\n",
    "\n",
    "# def merge_overlapping_blobs(df, max_disp = 20, min_radius = 5):\n",
    "#     # initialise an ID and label column:\n",
    "#     df['blob_ID'] = df.index\n",
    "\n",
    "#     # keep track of the z-plane\n",
    "#     zo = min(df.z.unique())\n",
    "#     max_rise = max_disp\n",
    "\n",
    "#     # generate a distance matrix\n",
    "#     dist = squareform(pdist(np.array(df.loc[:, np.array(['x', 'y', 'z'])], dtype = 'uint16'), 'euclidean'))\n",
    "\n",
    "#     # and a label dictionary\n",
    "#     relabel_dict = {}\n",
    "\n",
    "#     # for each z-plane in the data:\n",
    "#     # 1. determine the rise since the last z-plane\n",
    "#     # 2. find blobs that are at a dist < max_disp\n",
    "#     # 3. put the key-value (ID-label) pair in a dictionary\n",
    "\n",
    "#     # It is KEY to go progressively through the z-plane (not mapping over all at once).\n",
    "\n",
    "#     for i, z in enumerate(sorted(df.z.unique())[1:]):\n",
    "#         # match blob indices in the new plane to blob indices in the previous plane\n",
    "#         blob_match = {}\n",
    "\n",
    "#         # get the indices of the rows (in the previous z-plane) and columns (in this z-plane)\n",
    "#         ids_zo = np.array(df.index[df['z'] < z]) # rows in previous z-plane\n",
    "#         ids_z = np.array(df.index[df['z'] == z]) # columns in this z-plane\n",
    "\n",
    "#         # define a function to relabel IDs to merge with close and overlapping ID in previous planes\n",
    "#         def relabel_blob(ind, ids_zo = ids_zo, max_disp = max_disp, z = z):\n",
    "#             # subset distance matrix - we compare from the previous plane (zo) to this plane (z)\n",
    "#             sub = dist[ids_zo, ind]\n",
    "#             label_ind = ids_zo[np.where([overlaps and closest for overlaps, closest in zip((sub <= max_disp), (sub == np.min(sub)))])[0]]\n",
    "\n",
    "#             newlabel = df.loc[label_ind[0], 'blob_ID'] if (len(label_ind) > 0) else df.blob_ID[df.index == ind]\n",
    "#             return(str(int(newlabel)))\n",
    "\n",
    "#         for ind in ids_z:\n",
    "# #             print(f'index {str(ind)} will be labelled {str(int(relabel_blob(ind)))}')\n",
    "#             df.loc[ind, 'blob_ID'] = relabel_blob(ind)\n",
    "    \n",
    "#     return(df)\n",
    "\n",
    "# # then, summarize on only maximum intensity value\n",
    "# def render_max_intensities_df(merged_labels_df):\n",
    "# #     print(f'##**Make sure you applied function __merge_overlapping_blobs__ first!**##')\n",
    "    \n",
    "#     ## less typing\n",
    "#     df = merged_labels_df\n",
    "    \n",
    "#     ## initiate array with right size\n",
    "#     max_Idf = np.zeros(shape = (len(df.blob_ID.unique()),len(df.columns)))\n",
    "\n",
    "#     ## return line with max I for each blob\n",
    "#     for i, ID in enumerate([x for x in df.blob_ID.unique() if x != \"NA\"]):\n",
    "#         sub = df[(df['blob_ID'] == ID)]\n",
    "#         max_Idf[i, :] = sub[sub['I'] == np.max(sub['I'])]\n",
    "#         max_Idf[i, np.where(sub.columns == 'r_z')[0][0]] = np.sum(sub['r_z'])/2\n",
    "\n",
    "#     ## convert array to df and return\n",
    "#     return(pd.DataFrame(max_Idf, columns = merged_labels_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93fe1e23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Idf = df_s\n",
    "# img = np.mean(imstack, axis = 0)\n",
    "# ## open labels file\n",
    "# fig, axes = plt.subplots(1, 1)\n",
    "\n",
    "# ## normalize intensities\n",
    "# # rel_Is = map[x/np.max(list(imip[2]['I.mean'])) for x in list(imip[2]['I.mean'])]\n",
    "# img_inv = np.mean(imstack, axis = 0) - np.max(imstack, axis = 0)\n",
    "# # plt.imshow(img_inv, cmap = 'gray')\n",
    "\n",
    "# for i, cell in enumerate(Idf['ID']):\n",
    "#     x, y, lbl, area, r = Idf.loc[i, np.array(['x', 'y', 'ID', 'area', 'r'])]\n",
    "# #     color = 'red' if area > np.pi*r_min**2 else 'black'\n",
    "\n",
    "#         ## if you like to plot, use below code on an image\n",
    "#     plt.text(y = y, x = x, s = '+', color = 'black', fontsize = 'small', horizontalalignment='center',\n",
    "#      verticalalignment='center')\n",
    "# #     ## plot the blob + a margin\n",
    "#     c = plt.Circle((x, y),  r*1.6, color= 'black', linewidth=.3, linestyle = '--', fill=False)\n",
    "#     axes.add_patch(c)\n",
    "    \n",
    "# axes.imshow(img_inv, cmap = 'gray')\n",
    "# # axes.set_axis_off()\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0917552b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder = filedialog.askdirectory()\n",
    "# files = glob.glob(folder + '/*.czi')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a7b2aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7ce8e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## print so we know that reading the file worked\n",
    "# # print(f'# Processing file {i+1}/{len(files)} ({os.path.basename(file)})')\n",
    "\n",
    "# ## read file\n",
    "# imstack = readczi(file)\n",
    "# print(f' >>> read stack with {imstack.shape[0]} planes.')\n",
    "\n",
    "# ## fetch blobs from image\n",
    "# df, seq = get_img_and_Idf_from_imstack(imstack, sigma = sigma, threshold = threshold)\n",
    "# print(f' >>> found {df.shape[0]} nuclei (?) in image stack')\n",
    "\n",
    "# ## save as .csv file\n",
    "# # out_filename = folder + '/' + os.path.basename(file)[:-4] + '_objects.csv'\n",
    "# # df.to_csv(out_filename)\n",
    "# print(f' >>> written blobs to disk')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c35db5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "737d0104",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Processing file 1/16 (20240820-PXXX_vX-IBE954_21_1.czi)\n",
      " >>> read stack with 176 planes.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m >>> read stack with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimstack\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m planes.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m## fetch blobs from image\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m df, seq \u001b[38;5;241m=\u001b[39m \u001b[43mget_img_and_Idf_from_imstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimstack\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m >>> found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m nuclei (?) in image stack\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m## save as .csv file\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m, in \u001b[0;36mget_img_and_Idf_from_imstack\u001b[1;34m(imstack, sigma, threshold, split_ratio)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_img_and_Idf_from_imstack\u001b[39m(imstack, sigma, threshold, split_ratio \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m      2\u001b[0m     \n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m## take laplacian of the gaussian of the image - make sigma wide enough for good smoothing\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     img_lp \u001b[38;5;241m=\u001b[39m laplace(\u001b[43mgaussian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimstack\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m## threshold on percentile - very strict, yet slightly permissive threshold of 99.8\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     thresh \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mpercentile(img_lp, threshold)\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\skimage\\_shared\\utils.py:348\u001b[0m, in \u001b[0;36mdeprecate_multichannel_kwarg.__call__.<locals>.fixed_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    345\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchannel_axis\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m convert[kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmultichannel\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[0;32m    347\u001b[0m \u001b[38;5;66;03m# Call the function with the fixed arguments\u001b[39;00m\n\u001b[1;32m--> 348\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\skimage\\_shared\\filters.py:136\u001b[0m, in \u001b[0;36mgaussian\u001b[1;34m(image, sigma, output, mode, cval, multichannel, preserve_range, truncate, channel_axis)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(output\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39mfloating)):\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProvided output data type is not float\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mndi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgaussian_filter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncate\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\scipy\\ndimage\\_filters.py:379\u001b[0m, in \u001b[0;36mgaussian_filter\u001b[1;34m(input, sigma, order, output, mode, cval, truncate, radius, axes)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(axes) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m axis, sigma, order, mode, radius \u001b[38;5;129;01min\u001b[39;00m axes:\n\u001b[1;32m--> 379\u001b[0m         \u001b[43mgaussian_filter1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mradius\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mradius\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    381\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m output\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\scipy\\ndimage\\_filters.py:277\u001b[0m, in \u001b[0;36mgaussian_filter1d\u001b[1;34m(input, sigma, axis, order, output, mode, cval, truncate, radius)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;66;03m# Since we are calling correlate, not convolve, revert the kernel\u001b[39;00m\n\u001b[0;32m    276\u001b[0m weights \u001b[38;5;241m=\u001b[39m _gaussian_kernel1d(sigma, order, lw)[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m--> 277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcorrelate1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\scipy\\ndimage\\_filters.py:134\u001b[0m, in \u001b[0;36mcorrelate1d\u001b[1;34m(input, weights, axis, output, mode, cval, origin)\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvalid origin; origin must satisfy \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    131\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-(len(weights) // 2) <= origin <= \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    132\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(len(weights)-1) // 2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    133\u001b[0m mode \u001b[38;5;241m=\u001b[39m _ni_support\u001b[38;5;241m.\u001b[39m_extend_mode_to_code(mode)\n\u001b[1;32m--> 134\u001b[0m \u001b[43m_nd_image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorrelate1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m                      \u001b[49m\u001b[43morigin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## don't touch this cell!!\n",
    "sigma = 7\n",
    "threshold = 99.8\n",
    "\n",
    "folder = filedialog.askdirectory()\n",
    "files = glob.glob(folder + '/*.czi')\n",
    "    \n",
    "for i, file in enumerate(files):\n",
    "    ## print so we know that reading the file worked\n",
    "    print(f'# Processing file {i+1}/{len(files)} ({os.path.basename(file)})')\n",
    "    \n",
    "    ## read file\n",
    "    imstack = readczi(file)\n",
    "    print(f' >>> read stack with {imstack.shape[0]} planes.')\n",
    "    \n",
    "    ## fetch blobs from image\n",
    "    df, seq = get_img_and_Idf_from_imstack(imstack, sigma = sigma, threshold = threshold)\n",
    "    print(f' >>> found {df.shape[0]} nuclei (?) in image stack')\n",
    "    \n",
    "    ## save as .csv file\n",
    "    out_filename = folder + '/' + os.path.basename(file)[:-4] + '_objects.csv'\n",
    "    df.to_csv(out_filename)\n",
    "    print(f' >>> written blobs to disk')\n",
    "\n",
    "#     ## merge the blobs to a single intensity value (blobs are merged on how near they are)\n",
    "#     ## for so long that the dataframe gets compacter (shorter)\n",
    "#     c0 = len(df.index) ## initiate initial 'compactness'\n",
    "#     compacter = True ## set value to check whether it got better or not\n",
    "#     Idf = df\n",
    "#     while compacter == True:\n",
    "#         df_m = merge_overlapping_blobs(Idf, max_disp = 30)\n",
    "#         df_s = render_max_intensities_df(df_m)\n",
    "#         c = len(df_s.index)\n",
    "\n",
    "#         if c == c0:\n",
    "#             print(f'--- no more overlapping blobs')\n",
    "#             df_s = render_max_intensities_df(df_s)\n",
    "#             compacter = False\n",
    "#         else:\n",
    "#             print(f'--- discovered {c0 - c} more overlapping blobs')\n",
    "#             c0 = c\n",
    "#             Idf = df_s\n",
    "\n",
    "#     print(f' >>> rendered intensity data')\n",
    "\n",
    "#     ## save as .csv file\n",
    "#     df_filename = folder + '/' + os.path.basename(file)[:-4] + '_labeled_blobstack_v4.csv'\n",
    "#     df_s.to_csv(df_filename)\n",
    "#     print(f' >>> written intensity data to disk\\n\\n')\n",
    "\n",
    "    ## save tif of background subtracted frames\n",
    "#     seq_filename = folder + '/' + os.path.basename(file)[:-4] + '_seq_v3.tif'\n",
    "#     tf.imsave(seq_filename, seq)\n",
    "#     print(' >>> wrote background subtracted image sequence to disk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d3aee7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opencv-env",
   "language": "python",
   "name": "opencv-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
